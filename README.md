# Web-Crawler
“This web crawler is designed to scrape web pages, extract data, and store it in a structured format. Useful for data collection and analysis.” 	



## Demo

[https://drive.google.com/drive/u/0/folders/12NegXIaZQTTdqLAFnHTF992tRGY63fLO] (Click Me)

## About

Web Crawler is a Python-based tool designed to help users systematically browse and index web pages. With an intuitive graphical user interface (GUI) built using customtkinter, this tool allows users to start and monitor web crawling tasks easily.

## Purpose

The primary goal of the Web Crawler is to enable users to crawl websites to collect and index URLs, allowing for efficient data gathering and analysis. This tool is especially useful for web scraping, SEO analysis, and understanding website structures.
# Usage


## Requirements

- Python 3.x
- Libraries specified in `requirements.txt`


## Installation

To set up Web Crawler on your system:

-> Clone the Repository:

```bash
  git clone https://github.com/ZXOZXXX/web-crawler.git

```

-> Navigate to the Project Directory:

```bash
   cd web-crawler
```

-> Install Dependencies:
Ensure you have Python 3.x installed. Then, install the required libraries using pip:

```bash 
   pip install -r requirements.txt
```
## Running the Program

Run the Program:

```bash
  python code_1.py
```

✅ Enter a URL: In the GUI, input the URL you wish to crawl and click "Start Crawling".

✅ Monitor Progress: Use the GUI to track crawling progress and view results.
## Features

🟢🔴 Start and stop crawling through the GUI.

🪟 View queue and crawled URLs.

## Notes

🔗Ensure the URL you enter is valid.

🔳The program uses threading for concurrent crawling.
## License

[MIT](https://choosealicense.com/licenses/mit/)


## Support

For support, email:- apaliwal033@gmail.com 


## Authors

- [@ZXOZXXX](https://www.github.com/ZXOZXXX)

